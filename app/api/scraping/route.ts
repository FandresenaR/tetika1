import { NextRequest, NextResponse } from 'next/server';
import axios from 'axios';
import * as cheerio from 'cheerio';
import puppeteer, { Page } from 'puppeteer';

// Import MCP for enhanced web navigation
interface MCPNavigationResult {
  success: boolean;
  companies: CompanyData[];
  totalFound: number;
  method: string;
  error?: string;
}

interface CompanyData {
  name: string;
  website: string;
  description: string;
  logo?: string;
  employees?: string;
  industry?: string;
  location?: string;
  additionalData?: Record<string, string>;
}

// MCP-enhanced navigation function
async function performMCPNavigation(url: string, maxCompanies = 50): Promise<MCPNavigationResult> {
  try {
    console.log(`[MCP Navigation] Starting intelligent navigation for: ${url}`);
    
    // Step 1: Use MCP intelligent navigation tool
    const response = await fetch(`${process.env.NODE_ENV === 'development' ? 'http://localhost:3000' : ''}/api/mcp`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        tool: 'intelligent_navigation',
        args: {
          url: url,
          task: 'extract_companies',
          maxResults: maxCompanies,
          maxPages: 3
        }
      })
    });

    if (!response.ok) {
      throw new Error(`MCP intelligent navigation failed: ${response.status}`);
    }

    const mcpResult = await response.json();
    
    if (mcpResult.success && mcpResult.data?.content?.[0]?.text) {
      const navigationData = JSON.parse(mcpResult.data.content[0].text);
      
      if (navigationData.success && navigationData.companies && navigationData.companies.length > 0) {
        console.log(`[MCP Navigation] Successfully extracted ${navigationData.companies.length} companies via intelligent navigation`);
        return {
          success: true,
          companies: navigationData.companies,
          totalFound: navigationData.totalFound,
          method: navigationData.method
        };
      }
    }
    
    // Step 2: Fallback to direct company extraction
    console.log(`[MCP Navigation] Fallback to direct company extraction...`);
    const extractResponse = await fetch(`${process.env.NODE_ENV === 'development' ? 'http://localhost:3000' : ''}/api/mcp`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        tool: 'extract_company_data',
        args: {
          url: url,
          extractionMode: 'company_directory',
          maxResults: maxCompanies
        }
      })
    });

    if (extractResponse.ok) {
      const extractResult = await extractResponse.json();
      
      if (extractResult.success && extractResult.data?.content?.[0]?.text) {
        const extractData = JSON.parse(extractResult.data.content[0].text);
        
        if (extractData.success && extractData.companies && extractData.companies.length > 0) {
          console.log(`[MCP Navigation] Successfully extracted ${extractData.companies.length} companies via direct extraction`);
          return {
            success: true,
            companies: extractData.companies,
            totalFound: extractData.totalFound,
            method: extractData.method
          };
        }      }
    }
    
    // Step 3: Final fallback
    console.log(`[MCP Navigation] All MCP methods failed, returning empty result`);
    return {
      success: false,
      companies: [],
      totalFound: 0,
      method: 'MCP All Methods Failed',
      error: 'No structured company data found via any MCP method'
    };
    
  } catch (error) {
    console.error(`[MCP Navigation] Error:`, error);
    return {
      success: false,
      companies: [],
      totalFound: 0,
      method: 'MCP Navigation Error',
      error: error instanceof Error ? error.message : 'Unknown MCP error'
    };
  }
}

// Enhanced search function using multiple search engines
async function performEnhancedSearch(query: string, maxResults: number = 5): Promise<Array<{
  title: string;
  url: string;
  snippet: string;
  source: string;
}>> {
  const results: Array<{
    title: string;
    url: string;
    snippet: string;
    source: string;
  }> = [];

  try {
    // Try DuckDuckGo search first
    const duckDuckGoResults = await searchDuckDuckGo(query, Math.min(maxResults, 10));
    results.push(...duckDuckGoResults.slice(0, maxResults));
    
    // If we need more results, try additional sources
    if (results.length < maxResults) {
      const remainingNeeded = maxResults - results.length;
      
      // Try a general web search with Axios + Cheerio
      const webResults = await searchWeb(query, remainingNeeded);
      results.push(...webResults);
    }
  } catch (error) {
    console.error('Enhanced search error:', error);
    
    // Fallback: return some dummy results for testing
    results.push({
      title: `Search results for: ${query}`,
      url: 'https://example.com',
      snippet: `No results found for "${query}". This is a fallback result.`,
      source: 'fallback'
    });
  }

  return results.slice(0, maxResults);
}

// DuckDuckGo search implementation
async function searchDuckDuckGo(query: string, maxResults: number): Promise<Array<{
  title: string;
  url: string;
  snippet: string;
  source: string;
}>> {
  try {
    const searchUrl = `https://html.duckduckgo.com/html/?q=${encodeURIComponent(query)}`;
    const response = await axios.get(searchUrl, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
      },
      timeout: 10000
    });

    const $ = cheerio.load(response.data);
    const results: Array<{
      title: string;
      url: string;
      snippet: string;
      source: string;
    }> = [];

    $('.result').each((index, element) => {
      if (results.length >= maxResults) return false;
      
      const titleElement = $(element).find('.result__title a');
      const snippetElement = $(element).find('.result__snippet');
      
      const title = titleElement.text().trim();
      const url = titleElement.attr('href');
      const snippet = snippetElement.text().trim();
      
      if (title && url && snippet) {
        results.push({
          title,
          url: url.startsWith('//') ? 'https:' + url : url,
          snippet,
          source: 'DuckDuckGo'
        });
      }
    });

    return results;
  } catch (error) {
    console.error('DuckDuckGo search error:', error);
    return [];
  }
}

// Generic web search fallback
async function searchWeb(query: string, maxResults: number): Promise<Array<{
  title: string;
  url: string;
  snippet: string;
  source: string;
}>> {
  try {
    // This is a simple fallback that searches for the query terms on popular sites
    const searchTerms = query.split(' ').filter(term => term.length > 2);
    const results: Array<{
      title: string;
      url: string;
      snippet: string;
      source: string;
    }> = [];
    
    // Generate some relevant URLs based on query
    const baseUrls = [
      'https://wikipedia.org',
      'https://github.com',
      'https://stackoverflow.com',
      'https://medium.com',
      'https://dev.to'
    ];
    
    for (let i = 0; i < maxResults && i < baseUrls.length; i++) {
      results.push({
        title: `${searchTerms[0] || query} - ${baseUrls[i].split('//')[1]}`,
        url: `${baseUrls[i]}/search?q=${encodeURIComponent(query)}`,
        snippet: `Search results for "${query}" on ${baseUrls[i].split('//')[1]}`,
        source: 'Web Search'
      });
    }
    
    return results;
  } catch (error) {
    console.error('Web search error:', error);
    return [];
  }
}

interface StepData {
  targetUrl?: string;
  companiesFound?: number;
  scrapingMethod?: string;
  mcpSuccess?: boolean;
  error?: string;
  fallbackMethod?: string;
  searchResults?: number;
  extractedData?: unknown;
  analysisComplete?: boolean;
  foundSources?: number;
  companiesProcessed?: number;
  successfulExtractions?: number;
  engines?: string[];
  dataFields?: string[];
  totalAttempted?: number;
  // For report data
  companyData?: CompanyData[];
  query?: string;
  timestamp?: string;
  scrapingType?: string;
  summary?: {
    totalCompanies: number;
    successfulExtractions: number;
    failedExtractions: number;
    processingTime: string;
  };
  sources?: Array<{
    url: string;
    title: string;
    companiesFound: number;
    processingTime: number;
  }>;
  analysis?: unknown;
  rawContent?: unknown;
}

interface ReportData {
  analysis?: {
    companies?: CompanyData[];
    totalFound?: number;
    method?: string;
  };
  metadata?: {
    timestamp: string;
    processingTime?: number;
    sourcesProcessed?: number;
  };
}

interface ScrapingStep {
  id: string;
  title: string;
  description: string;
  sources: string[];
  timestamp: number;
  status: 'in-progress' | 'completed' | 'error';
  data?: StepData;
}

interface ScrapingRequest {
  query: string;
  mode: 'deep-scraping' | 'quick-scraping';
  maxSources?: number;
  includeAnalysis?: boolean;
}

interface ScrapingResponse {
  success: boolean;
  steps: ScrapingStep[];
  reportData?: ReportData;
  error?: string;
  // New MCP-related fields
  companies?: CompanyData[];
  mcpMethod?: string;
  totalFound?: number;
}

class ScrapingSystem {
  private steps: ScrapingStep[] = [];
  private stepCounter = 0;

  addStep(title: string, description: string, sources: string[] = []): string {
    const stepId = `step_${++this.stepCounter}_${Date.now()}`;
    const step: ScrapingStep = {
      id: stepId,
      title,
      description,
      sources,
      timestamp: Date.now(),
      status: 'in-progress'
    };
    this.steps.push(step);
    return stepId;
  }

  updateStep(stepId: string, status: 'completed' | 'error', data?: StepData) {
    const step = this.steps.find(s => s.id === stepId);
    if (step) {
      step.status = status;
      if (data) step.data = data;
    }
  }

  addSource(stepId: string, source: string) {
    const step = this.steps.find(s => s.id === stepId);
    if (step && !step.sources.includes(source)) {
      step.sources.push(source);
    }
  }

  getSteps(): ScrapingStep[] {
    return [...this.steps];
  }

  clear() {
    this.steps = [];
    this.stepCounter = 0;
  }
}

// Enhanced direct URL scraping with dynamic content loading
async function performDirectUrlScraping(url: string, maxCompanies = 50): Promise<CompanyData[]> {
  let browser = null;
  
  try {
    console.log(`[DirectScraping] Launching browser for: ${url}`);
    browser = await puppeteer.launch({ 
      headless: true,
      args: [
        '--no-sandbox', 
        '--disable-setuid-sandbox', 
        '--disable-dev-shm-usage',
        '--disable-web-security',
        '--disable-features=VizDisplayCompositor',
        '--disable-blink-features=AutomationControlled'
      ],
      timeout: 20000
    });
    
    const page = await browser.newPage();
    
    // Set more realistic browser headers
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');
    await page.setExtraHTTPHeaders({
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
      'Accept-Language': 'en-US,en;q=0.5',
      'Accept-Encoding': 'gzip, deflate, br',
      'Connection': 'keep-alive',
      'Upgrade-Insecure-Requests': '1'
    });
    
    // Try multiple navigation strategies
    console.log(`[DirectScraping] Navigating to: ${url}`);
    
    try {
      // Strategy 1: Try with networkidle0 and longer timeout
      await page.goto(url, { 
        waitUntil: 'networkidle0', 
        timeout: 45000 
      });
    } catch (firstError) {
      console.warn(`[DirectScraping] First strategy failed: ${firstError instanceof Error ? firstError.message : String(firstError)}`);
      
      try {
        // Strategy 2: Try with domcontentloaded only
        await page.goto(url, { 
          waitUntil: 'domcontentloaded', 
          timeout: 20000 
        });
      } catch (secondError) {
        console.warn(`[DirectScraping] Second strategy failed: ${secondError instanceof Error ? secondError.message : String(secondError)}`);
        
        // Strategy 3: Try with basic axios scraping as fallback
        console.log(`[DirectScraping] Falling back to basic HTTP scraping`);
        await browser.close();
        return await performBasicHttpScraping(url, maxCompanies);
      }
    }
      // Wait for content to load and try to detect dynamic content
    await new Promise(resolve => setTimeout(resolve, 5000));
    
    // Try to scroll to trigger lazy loading
    await page.evaluate(() => {
      window.scrollTo(0, document.body.scrollHeight / 2);    });
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    await page.evaluate(() => {
      window.scrollTo(0, document.body.scrollHeight);    });
    await new Promise(resolve => setTimeout(resolve, 3000));
    
    // Check if it's a partners/companies listing page
    const isPartnersPage = url.includes('partners') || url.includes('companies');
    
    if (isPartnersPage) {
      return await scrapePartnersPage(page, maxCompanies);
    } else {
      // For other pages, use general scraping
      return await scrapeGeneralPage(page);
    }
  } catch (error) {
    console.error(`[DirectScraping] Error: ${error instanceof Error ? error.message : String(error)}`);
    
    // Ultimate fallback: try basic HTTP scraping
    try {
      console.log(`[DirectScraping] Trying basic HTTP fallback`);
      return await performBasicHttpScraping(url, maxCompanies);
    } catch (fallbackError) {      console.error(`[DirectScraping] All strategies failed: ${fallbackError instanceof Error ? fallbackError.message : String(fallbackError)}`);
      throw new Error(`Failed to scrape ${url}: ${error instanceof Error ? error.message : String(error)}. Fallback also failed: ${fallbackError instanceof Error ? fallbackError.message : String(fallbackError)}`);
    }
  } finally {
    if (browser) {
      try {
        await browser.close();
      } catch (closeError) {
        console.warn(`[DirectScraping] Error closing browser: ${closeError instanceof Error ? closeError.message : String(closeError)}`);
      }
    }
  }
}

// Basic HTTP scraping fallback using axios + cheerio
async function performBasicHttpScraping(url: string, maxCompanies = 50): Promise<CompanyData[]> {
  try {
    console.log(`[BasicScraping] Attempting HTTP scraping for: ${url}`);
    
    const response = await axios.get(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
      },
      timeout: 15000,
      maxRedirects: 5
    });    const $ = cheerio.load(response.data);
    const companies: CompanyData[] = [];
    
    // Common selectors for company/partner listings
    const companySelectors = [
      '.partner-card',
      '.company-card', 
      '.partner-item',
      '.company-item',
      '[data-partner]',
      '[data-company]',
      '.grid-item',
      '.card',
      'article',
      '.partner',
      '.sponsor',
      '.exhibitor',
      '.company',
      '.member'
    ];
    
    // Try each selector
    for (const selector of companySelectors) {
      const elements = $(selector);
      console.log(`[BasicScraping] Found ${elements.length} elements with selector: ${selector}`);
      
      if (elements.length > 0) {
        elements.each((index, element) => {
          if (companies.length >= maxCompanies) return false;
          
          const $el = $(element);
          const company = {
            name: '',
            website: '',
            description: '',
            logo: '',
            employees: 'Unknown',
            industry: 'Unknown',
            location: 'Unknown'
          };
          
          // Extract company name
          const nameSelectors = ['h1', 'h2', 'h3', 'h4', '.name', '.title', '.company-name', '.partner-name'];
          for (const nameSel of nameSelectors) {
            const nameEl = $el.find(nameSel).first();
            if (nameEl.length && nameEl.text().trim()) {
              company.name = nameEl.text().trim();
              break;
            }
          }
          
          // Extract website
          const linkEl = $el.find('a[href]').first();
          if (linkEl.length) {
            const href = linkEl.attr('href');
            if (href && (href.startsWith('http') || href.startsWith('www'))) {
              company.website = href;
            } else if (href && href.startsWith('/')) {
              company.website = new URL(url).origin + href;
            }
          }
          
          // Extract logo
          const logoEl = $el.find('img').first();
          if (logoEl.length) {
            company.logo = logoEl.attr('src') || '';
          }
          
          // Extract description
          const descSelectors = ['.description', '.summary', '.bio', 'p'];
          for (const descSel of descSelectors) {
            const descEl = $el.find(descSel).first();
            if (descEl.length && descEl.text().trim()) {
              company.description = descEl.text().trim();
              break;
            }
          }
          
          // Only add if we have at least a name
          if (company.name && company.name.length > 2) {
            companies.push(company);
          }
        });
        
        if (companies.length > 0) {
          break; // Found companies with this selector
        }
      }
    }
    
    // If no structured companies found, try to extract from links
    if (companies.length === 0) {
      console.log(`[BasicScraping] No structured companies found, extracting from links`);
      $('a[href]').each((index, element) => {
        if (companies.length >= maxCompanies) return false;
        
        const $link = $(element);
        const text = $link.text().trim();
        const href = $link.attr('href');
        
        if (text && href && text.length > 2 && text.length < 100) {
          // Skip common navigation links
          const skipTexts = ['home', 'about', 'contact', 'login', 'register', 'menu', 'search', 'more', 'view', 'see'];
          if (!skipTexts.some(skip => text.toLowerCase().includes(skip))) {
            companies.push({
              name: text,
              website: href.startsWith('http') ? href : new URL(url).origin + href,
              description: 'Extracted from link',
              logo: '',
              employees: 'Unknown',
              industry: 'Unknown',
              location: 'Unknown'
            });
          }
        }
      });
    }
    
    console.log(`[BasicScraping] Extracted ${companies.length} companies`);
    return companies;
      } catch (error) {
    console.error(`[BasicScraping] Error:`, error instanceof Error ? error.message : 'Unknown error');
    throw error;
  }
}

// Specialized scraping for partners/companies pages
async function scrapePartnersPage(page: Page, maxCompanies: number): Promise<CompanyData[]> {
  const companies: CompanyData[] = [];
  
  // Common selectors for company listings
  const companySelectors = [
    '.partner-card',
    '.company-card', 
    '.partner-item',
    '.company-item',
    '[data-partner]',
    '[data-company]',
    '.grid-item',
    '.card',
    'article',
    '.partner',
    '.sponsor',
    '.exhibitor'
  ];
  
  // Try to find companies using different selectors
  let companiesFound = false;
  
  for (const selector of companySelectors) {
    try {
      const elements = await page.$$(selector);
      if (elements.length > 0) {
        console.log(`[PartnersPage] Found ${elements.length} companies using selector: ${selector}`);
        
        for (let i = 0; i < Math.min(elements.length, maxCompanies); i++) {
          try {
            const companyData = await extractCompanyData(page, elements[i]);            if (companyData && companyData.name) {
              companies.push(companyData);
            }
          } catch (error) {
            console.warn(`[PartnersPage] Error extracting company ${i}:`, error);
          }
        }
        
        companiesFound = true;
        break;
      }    } catch (selectorError) {
      console.warn(`[PartnersPage] Selector ${selector} failed:`, selectorError);
    }
  }
  
  // If no companies found with cards, try links approach
  if (!companiesFound) {
    console.log('[PartnersPage] Trying link-based extraction...');
    const partialCompanies = await extractCompaniesFromLinks(page, maxCompanies);
    // Filter out incomplete data and convert to CompanyData
    const validCompanies = partialCompanies
      .filter((company): company is CompanyData => 
        Boolean(company.name && company.website && company.description)
      );
    companies.push(...validCompanies);
  }
  
  // Try to load more content if needed
  if (companies.length < maxCompanies) {
    await loadMoreContent(page);
    
    // Re-extract after loading more
    for (const selector of companySelectors) {
      try {
        const elements = await page.$$(selector);
        if (elements.length > companies.length) {
          console.log(`[PartnersPage] Found ${elements.length - companies.length} more companies after loading`);
          
          for (let i = companies.length; i < Math.min(elements.length, maxCompanies); i++) {
            try {
              const companyData = await extractCompanyData(page, elements[i]);              if (companyData && companyData.name && !companies.some((c: CompanyData) => c.name === companyData.name)) {
                companies.push(companyData);
              }
            } catch (error) {
              console.warn(`[PartnersPage] Error extracting additional company ${i}:`, error);
            }
          }          break;
        }
      } catch {
        continue;
      }
    }
  }
  
  return companies;
}

// Extract company data from a single element
async function extractCompanyData(page: Page, element: unknown): Promise<CompanyData | null> {
  try {    const companyData = await page.evaluate((el) => {
      const data: Partial<CompanyData> = {
        name: '',
        website: '',
        description: '',
        logo: '',
        employees: '',
        industry: '',
        location: ''
      };
      
      // Cast to Element for TypeScript
      const element = el as Element;
      
      // Extract company name
      const nameSelectors = ['h1', 'h2', 'h3', 'h4', '.name', '.title', '.company-name', '.partner-name'];
      for (const sel of nameSelectors) {
        const nameEl = element.querySelector(sel);
        if (nameEl && nameEl.textContent?.trim()) {
          data.name = nameEl.textContent.trim();
          break;
        }
      }
      
      // Extract website
      const linkEl = element.querySelector('a[href]');
      if (linkEl) {
        const href = linkEl.getAttribute('href');
        if (href && (href.startsWith('http') || href.startsWith('www'))) {
          data.website = href;
        } else if (href && href.startsWith('/')) {
          data.website = window.location.origin + href;
        }
      }
      
      // Extract logo
      const logoEl = element.querySelector('img');
      if (logoEl) {
        data.logo = logoEl.getAttribute('src') || '';
      }
      
      // Extract description
      const descSelectors = ['.description', '.summary', '.bio', 'p'];
      for (const sel of descSelectors) {
        const descEl = element.querySelector(sel);
        if (descEl && descEl.textContent?.trim()) {
          data.description = descEl.textContent.trim();
          break;
        }
      }
      
      return data;
    }, element);
      // Convert Partial<CompanyData> to CompanyData with defaults
    const result: CompanyData = {
      name: companyData.name || '',
      website: companyData.website || '',
      description: companyData.description || '',
      logo: companyData.logo || '',
      employees: companyData.employees || 'Unknown',
      industry: companyData.industry || 'Unknown',
      location: companyData.location || 'Unknown'
    };
    
    // If we have a link to company details, try to get more info
    if (result.website && result.website.includes(page.url().split('/')[2])) {
      try {
        const detailedData = await getCompanyDetails(page, result.website);
        return { ...result, ...detailedData };
      } catch (detailError) {
        console.warn('[CompanyData] Error getting details:', detailError);
      }
    }
    
    return result;  } catch (extractError) {
    console.warn('[CompanyData] Error extracting:', extractError);
    return { name: '', website: '', description: '' };
  }
}

// Get detailed company information by following links
async function getCompanyDetails(page: Page, companyUrl: string): Promise<Partial<CompanyData>> {
  try {
    const newPage = await page.browser().newPage();
    await newPage.goto(companyUrl, { waitUntil: 'networkidle2', timeout: 15000 });
    
    const details = await newPage.evaluate(() => {
      const data: Partial<CompanyData> = {};
      
      // Look for employee count
      const employeePatterns = [
        /(\d+[\s,]*\d*)\s*employees?/i,
        /team\s*of\s*(\d+)/i,
        /(\d+)\s*people/i,
        /size[:\s]*(\d+)/i
      ];
      
      const text = document.body.textContent || '';
      for (const pattern of employeePatterns) {
        const match = text.match(pattern);
        if (match) {
          data.employees = match[1].replace(/[,\s]/g, '');
          break;
        }
      }
      
      // Look for industry/sector
      const industryKeywords = ['industry', 'sector', 'domain', 'field', 'specialization'];
      for (const keyword of industryKeywords) {
        const el = document.querySelector(`[data-${keyword}], .${keyword}, #${keyword}`);
        if (el && el.textContent) {
          data.industry = el.textContent.trim();
          break;
        }
      }
      
      // Look for location
      const locationKeywords = ['location', 'address', 'city', 'country', 'headquarters'];
      for (const keyword of locationKeywords) {
        const el = document.querySelector(`[data-${keyword}], .${keyword}, #${keyword}`);
        if (el && el.textContent) {
          data.location = el.textContent.trim();
          break;
        }
      }
      
      return data;
    });
    
    await newPage.close();
    return details;
  } catch (error) {
    console.warn('[CompanyDetails] Error:', error);
    return {};
  }
}

// Extract companies from links when no structured data is available
async function extractCompaniesFromLinks(page: Page, maxCompanies: number): Promise<Partial<CompanyData>[]> {
  const companies = await page.evaluate((max: number) => {
    const links = Array.from(document.querySelectorAll('a[href]'));
    const companies: Partial<CompanyData>[] = [];
    
    for (const link of links) {
      if (companies.length >= max) break;
      
      const href = (link as HTMLAnchorElement).href;
      const text = link.textContent?.trim();
      
      if (text && href && text.length > 2 && text.length < 100) {
        // Skip navigation links
        if (!['home', 'about', 'contact', 'login', 'register', 'menu', 'search'].includes(text.toLowerCase())) {
          companies.push({
            name: text,
            website: href,
            description: 'Extracted from link',
            employees: 'Unknown',
            industry: 'Unknown',
            location: 'Unknown'
          });
        }
      }
    }
    
    return companies;
  }, maxCompanies);
  
  return companies;
}

// Load more content (infinite scroll, pagination, etc.)
async function loadMoreContent(page: Page): Promise<void> {
  try {
    // Try different methods to load more content
    
    // Method 1: Look for "Load More" button
    const loadMoreSelectors = [
      'button[class*="load"]',
      'button[class*="more"]',
      'a[class*="load"]',
      'a[class*="more"]',
      '.load-more',
      '.show-more',
      '.btn-load-more'
    ];
    
    for (const selector of loadMoreSelectors) {
      try {
        const button = await page.$(selector);        if (button) {          console.log(`[LoadMore] Clicking load more button: ${selector}`);
          await button.click();
          await new Promise(resolve => setTimeout(resolve, 2000));
          return;
        }
      } catch {
        continue;
      }
    }
      // Method 2: Try infinite scroll
    console.log('[LoadMore] Attempting infinite scroll...');
    await page.evaluate(() => {
      window.scrollTo(0, document.body.scrollHeight);
    });
    await new Promise(resolve => setTimeout(resolve, 3000));
    
    // Method 3: Look for pagination
    const paginationSelectors = [
      '.pagination a[href]',
      '.pager a[href]',
      'a[class*="next"]',
      'button[class*="next"]'
    ];
    
    for (const selector of paginationSelectors) {
      try {
        const nextButton = await page.$(selector);
        if (nextButton) {          console.log(`[LoadMore] Clicking pagination: ${selector}`);          await nextButton.click();
          await new Promise(resolve => setTimeout(resolve, 2000));
          return;
        }
      } catch {
        continue;
      }
    }
  } catch (error) {
    console.warn('[LoadMore] Error loading more content:', error);
  }
}

// General page scraping for non-partner pages
async function scrapeGeneralPage(page: Page): Promise<CompanyData[]> {
  const content = await page.evaluate(() => {
    return {
      title: document.title,
      url: window.location.href,
      text: document.body.textContent?.substring(0, 5000) || '',
      links: Array.from(document.querySelectorAll('a[href]')).slice(0, 20).map(a => ({
        text: (a as HTMLAnchorElement).textContent?.trim(),
        href: (a as HTMLAnchorElement).href
      }))
    };
  });
  
  // Convert general content to CompanyData format for consistency
  return [{
    name: content.title || 'Unknown',
    website: content.url,
    description: content.text.substring(0, 200) + '...',
    logo: '',
    employees: 'Unknown',
    industry: 'Unknown',
    location: 'Unknown'
  }];
}

// Enhanced content extraction from URLs
async function extractContentFromUrl(url: string): Promise<{ title: string; content: string; metadata: Record<string, unknown> }> {
  try {
    const response = await axios.get(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
      },
      timeout: 15000,
      maxRedirects: 5
    });

    const $ = cheerio.load(response.data);
    
    // Extract title
    const title = $('title').text().trim() || 
                  $('h1').first().text().trim() || 
                  'No title found';

    // Extract main content
    const contentSelectors = [
      'article',
      '[role="main"]',
      '.content',
      '.post-content',
      '.entry-content',
      '.article-content',
      '#content',
      'main'
    ];

    let content = '';
    for (const selector of contentSelectors) {
      const element = $(selector);
      if (element.length > 0) {
        // Remove script and style elements
        element.find('script, style, nav, footer, aside, .advertisement, .ads').remove();
        content = element.text().replace(/\s+/g, ' ').trim();
        if (content.length > 100) break;
      }
    }

    // Fallback to body content if no main content found
    if (!content || content.length < 100) {
      $('script, style, nav, footer, aside, .advertisement, .ads').remove();
      content = $('body').text().replace(/\s+/g, ' ').trim();
    }

    // Extract metadata
    const metadata = {
      url,
      wordCount: content.split(' ').length,
      description: $('meta[name="description"]').attr('content') || '',
      keywords: $('meta[name="keywords"]').attr('content') || '',
      author: $('meta[name="author"]').attr('content') || '',
      publishDate: $('meta[property="article:published_time"]').attr('content') || 
                   $('meta[name="date"]').attr('content') || '',
      language: $('html').attr('lang') || 'unknown'
    };

    return {
      title,
      content: content.substring(0, 5000), // Limit content length
      metadata
    };
  } catch (error) {
    throw new Error(`Failed to extract content from ${url}: ${error}`);
  }
}

// Analyze extracted content for insights
function analyzeContent(contents: Array<{ title: string; content: string; url: string }>): Record<string, unknown> {
  const analysis = {
    totalSources: contents.length,
    totalWords: 0,
    commonKeywords: {},
    topics: [],
    sentiment: 'neutral',
    sources: contents.map(c => ({
      url: c.url,
      title: c.title,
      wordCount: c.content.split(' ').length
    }))
  };

  // Word frequency analysis
  const allWords: string[] = [];
  contents.forEach(content => {
    const words = content.content.toLowerCase()
      .replace(/[^\w\s]/g, '')
      .split(/\s+/)
      .filter(word => word.length > 3 && !['this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'were', 'said', 'each', 'which', 'their', 'time', 'would', 'there', 'could', 'other'].includes(word));
    
    allWords.push(...words);
    analysis.totalWords += words.length;
  });

  // Count word frequencies
  const wordCounts: Record<string, number> = {};
  allWords.forEach(word => {
    wordCounts[word] = (wordCounts[word] || 0) + 1;
  });

  // Get top keywords
  analysis.commonKeywords = Object.entries(wordCounts)
    .sort(([,a], [,b]) => b - a)
    .slice(0, 20)
    .reduce((obj, [word, count]) => ({ ...obj, [word]: count }), {});

  return analysis;
}

// Analyze company data for business insights
function analyzeCompanyData(companies: Array<{ metadata?: Record<string, unknown>; content: string; title: string }>): Record<string, unknown> {
  const analysis = {
    totalCompanies: companies.length,
    industryDistribution: {} as Record<string, number>,
    employeeRanges: {
      'startup (1-50)': 0,
      'small (51-200)': 0,
      'medium (201-1000)': 0,
      'large (1001-5000)': 0,
      'enterprise (5000+)': 0,
      'unknown': 0
    },
    websiteStatus: {
      hasWebsite: 0,
      noWebsite: 0
    },
    locationDistribution: {} as Record<string, number>,
    dataQuality: {
      completeProfiles: 0,
      partialProfiles: 0,
      incompleteProfiles: 0
    }
  };

  companies.forEach(company => {
    const metadata = company.metadata || {};
      // Industry analysis
    if (metadata.industry && metadata.industry !== 'Unknown') {
      const industry = (metadata.industry as string).toLowerCase();
      analysis.industryDistribution[industry] = (analysis.industryDistribution[industry] || 0) + 1;
    }
    
    // Employee count analysis
    if (metadata.employees && metadata.employees !== 'Unknown') {
      const empCount = parseInt((metadata.employees as string).replace(/[,\s]/g, ''));
      if (!isNaN(empCount)) {
        if (empCount <= 50) analysis.employeeRanges['startup (1-50)']++;
        else if (empCount <= 200) analysis.employeeRanges['small (51-200)']++;
        else if (empCount <= 1000) analysis.employeeRanges['medium (201-1000)']++;
        else if (empCount <= 5000) analysis.employeeRanges['large (1001-5000)']++;
        else analysis.employeeRanges['enterprise (5000+)']++;
      } else {
        analysis.employeeRanges.unknown++;
      }
    } else {
      analysis.employeeRanges.unknown++;
    }
    
    // Website analysis
    if (metadata.website && metadata.website !== 'No website' && (metadata.website as string).startsWith('http')) {
      analysis.websiteStatus.hasWebsite++;
    } else {
      analysis.websiteStatus.noWebsite++;
    }
    
    // Location analysis
    if (metadata.location && metadata.location !== 'Unknown') {
      const location = (metadata.location as string).toLowerCase();
      analysis.locationDistribution[location] = (analysis.locationDistribution[location] || 0) + 1;
    }
    
    // Data quality assessment
    const fields = [metadata.name, metadata.website, metadata.employees, metadata.industry, metadata.location];
    const filledFields = fields.filter(field => field && field !== 'Unknown' && field !== 'No website').length;
    
    if (filledFields >= 4) analysis.dataQuality.completeProfiles++;
    else if (filledFields >= 2) analysis.dataQuality.partialProfiles++;
    else analysis.dataQuality.incompleteProfiles++;
  });

  return analysis;
}

export async function POST(request: NextRequest): Promise<NextResponse> {
  // Set a reasonable timeout for the entire operation (90 seconds)
  const timeoutPromise = new Promise((_, reject) => {
    setTimeout(() => reject(new Error('Request timeout after 90 seconds')), 90000);
  });
  const mainProcess = async () => {
    try {
      console.log('[POST] Parsing request body...');
      const body: ScrapingRequest = await request.json();
      console.log('[POST] Request body parsed:', JSON.stringify(body, null, 2));
      
      const { query, mode = 'quick-scraping', maxSources = 5, includeAnalysis = true } = body;

      if (!query) {
        console.log('[POST] Error: Query is missing');
        return NextResponse.json({
          success: false,
          error: 'Query is required'
        }, { status: 400 });
      }

      console.log(`[POST] Starting scraping with query: ${query}, mode: ${mode}`);
      
      const scraper = new ScrapingSystem();
      const results: ScrapingResponse = {
        success: true,
        steps: [],
        reportData: undefined
      };

      try {
        // Step 1: Initialize scraping
        const initStepId = scraper.addStep(
          'ðŸš€ Initializing Scraping System',
          `Starting ${mode} for query: "${query}"`
        );
        scraper.updateStep(initStepId, 'completed');

        // Step 2: Search for sources OR direct URL scraping
        let searchResults: Array<{
          title?: string;
          url?: string;
          source?: string;
          snippet?: string;
          name?: string;
          website?: string;
          description?: string;
          employees?: string;
          industry?: string;
          location?: string;
        }> = [];
        let isDirectUrl = false;
        
        // Check if query is a direct URL
        const urlRegex = /https?:\/\/[^\s]+/i;
        const urlMatch = query.match(urlRegex);
          if (urlMatch) {
          isDirectUrl = true;
          const targetUrl = urlMatch[0];
          const searchStepId = scraper.addStep(
            'ðŸŽ¯ MCP-Enhanced URL Analysis',
            `Using MCP intelligent navigation for: ${targetUrl}`
          );
          
          try {
            console.log(`[POST] Starting MCP navigation for: ${targetUrl}`);
            
            // First try MCP navigation
            const mcpResult = await performMCPNavigation(targetUrl, maxSources);
              if (mcpResult.success && mcpResult.companies.length > 0) {
              // Convert MCP results to the expected format
              searchResults = mcpResult.companies.map(company => ({
                name: company.name,
                website: company.website,
                description: company.description,
                employees: company.employees || 'Unknown',
                industry: company.industry || 'Unknown',
                location: company.location || 'Unknown',
                logo: company.logo || '',
                ...company.additionalData
              }));
              
              scraper.addSource(searchStepId, targetUrl);
              scraper.updateStep(searchStepId, 'completed', {
                targetUrl,
                companiesFound: searchResults.length,
                scrapingMethod: mcpResult.method,
                mcpSuccess: true
              });
              
              // Add companies directly to results for analysis
              results.companies = mcpResult.companies;
              results.mcpMethod = mcpResult.method;
              results.totalFound = mcpResult.totalFound;            } else {
              // Fallback to Puppeteer if MCP fails
              console.log(`[POST] MCP failed (${mcpResult.error || 'Unknown error'}), falling back to Puppeteer`);
              scraper.updateStep(searchStepId, 'error', { 
                error: mcpResult.error || 'MCP navigation failed',
                fallbackMethod: 'Puppeteer' 
              });
              
              // Update step for Puppeteer fallback
              const fallbackStepId = scraper.addStep(
                'ðŸ”„ Puppeteer Fallback',
                `MCP failed, trying Puppeteer scraping for: ${targetUrl}`
              );
              
              try {
                const puppeteerResults = await performDirectUrlScraping(targetUrl, maxSources);
                
                // Convert Puppeteer results to expected format
                searchResults = puppeteerResults.map(company => ({
                  name: company.name,
                  website: company.website,
                  description: company.description,
                  employees: company.employees || 'Unknown',
                  industry: company.industry || 'Unknown',
                  location: company.location || 'Unknown',
                  logo: company.logo || '',
                  ...company.additionalData
                }));
                
                scraper.addSource(fallbackStepId, targetUrl);
                scraper.updateStep(fallbackStepId, 'completed', {
                  targetUrl,
                  companiesFound: searchResults.length,
                  scrapingMethod: 'Puppeteer Fallback'
                });
                
                // Add companies to results
                results.companies = puppeteerResults;
                results.mcpMethod = 'Puppeteer Fallback';
                results.totalFound = puppeteerResults.length;
                
              } catch (puppeteerError) {
                console.error('Puppeteer fallback also failed:', puppeteerError);
                scraper.updateStep(fallbackStepId, 'error', { error: (puppeteerError as Error).message });
                
                // Final fallback: try basic HTTP scraping
                const basicStepId = scraper.addStep(
                  'ðŸŒ Basic HTTP Fallback',
                  `All advanced methods failed, trying basic HTTP scraping for: ${targetUrl}`
                );
                
                try {
                  const basicResults = await performBasicHttpScraping(targetUrl, maxSources);
                  
                  searchResults = basicResults.map(company => ({
                    name: company.name,
                    website: company.website,
                    description: company.description,
                    employees: company.employees || 'Unknown',
                    industry: company.industry || 'Unknown',
                    location: company.location || 'Unknown',
                    logo: company.logo || '',
                    ...company.additionalData
                  }));
                  
                  scraper.addSource(basicStepId, targetUrl);
                  scraper.updateStep(basicStepId, 'completed', {
                    targetUrl,
                    companiesFound: searchResults.length,
                    scrapingMethod: 'Basic HTTP Fallback'
                  });
                  
                  results.companies = basicResults;
                  results.mcpMethod = 'Basic HTTP Fallback';
                  results.totalFound = basicResults.length;
                  
                } catch (finalError) {
                  console.error('All scraping methods failed:', finalError);
                  scraper.updateStep(basicStepId, 'error', { error: (finalError as Error).message });
                  
                  // If all methods fail, still continue with search-based approach
                  console.log('[POST] All direct scraping methods failed, switching to search-based approach');
                  isDirectUrl = false;
                }
              }
            }
          } catch (error) {
            console.error('Both MCP and Puppeteer failed:', error);
            scraper.updateStep(searchStepId, 'error', { error: (error as Error).message });
            
            // Fallback to search if direct URL fails completely
            isDirectUrl = false;
          }
        }
        
        if (!isDirectUrl || searchResults.length === 0) {
          // Original search approach
          const searchStepId = scraper.addStep(
            'ðŸ” Searching for Sources',
            `Searching multiple engines for "${query}" (max ${maxSources} sources)`
          );

          searchResults = await performEnhancedSearch(query, maxSources);
          
          searchResults.forEach(result => {
            if (result.url) {
              scraper.addSource(searchStepId, result.url);
            }
          });

          scraper.updateStep(searchStepId, 'completed', {
            foundSources: searchResults.length,
            engines: ['DuckDuckGo', 'Searx']
          });
        }

        // Step 3: Extract content from sources OR process direct data
        let extractedContents = [];
        
        if (isDirectUrl && searchResults.length > 0) {
          // For direct URL scraping, data is already extracted
          const processStepId = scraper.addStep(
            'ðŸ¢ Processing Company Data',
            `Processing ${searchResults.length} companies from direct URL scraping`
          );
          
          extractedContents = searchResults.map(company => ({
            title: company.name || 'Unknown Company',
            content: `Company: ${company.name}\nWebsite: ${company.website}\nEmployees: ${company.employees}\nIndustry: ${company.industry}\nLocation: ${company.location}\nDescription: ${company.description}`,
            url: company.website || 'No website',
            metadata: {
              ...company,
              extractionMethod: 'Direct Company Data',
              language: 'en'
            }
          }));
          
          scraper.updateStep(processStepId, 'completed', {
            companiesProcessed: extractedContents.length,
            dataFields: ['name', 'website', 'employees', 'industry', 'location', 'description']
          });
        } else {
          // Original content extraction for search results
          const extractStepId = scraper.addStep(
            'ðŸ“„ Extracting Content',
            `Extracting and processing content from ${searchResults.length} sources`
          );

          for (const result of searchResults) {
            if (result.url) {
              try {
                const content = await extractContentFromUrl(result.url);
                extractedContents.push({
                  ...content,
                  url: result.url,
                  searchSnippet: result.snippet
                });
              } catch (error) {
                console.error(`Failed to extract from ${result.url}:`, error);
              }
            }
          }

          scraper.updateStep(extractStepId, 'completed', {
            successfulExtractions: extractedContents.length,
            totalAttempted: searchResults.length
          });
        }

        // Step 4: Analyze content (if requested)
        let analysis = null;
        if (includeAnalysis && extractedContents.length > 0) {
          const analysisStepId = scraper.addStep(
            'ðŸ“Š Analyzing Content',
            isDirectUrl ? 
              'Performing company analysis: industry distribution, employee ranges, and data insights' :
              'Performing keyword analysis, topic extraction, and content insights'
          );

          if (isDirectUrl) {
            analysis = analyzeCompanyData(extractedContents);
          } else {
            analysis = analyzeContent(extractedContents);
          }
          
          scraper.updateStep(analysisStepId, 'completed', analysis);
        }

        // Step 5: Generate report
        const reportStepId = scraper.addStep(
          'ðŸ“ Generating Report',
          'Compiling final report with all extracted data and analysis'
        );

        const reportData = {
          query,
          timestamp: new Date().toISOString(),
          scrapingType: isDirectUrl ? 'Direct URL Analysis' : 'Search-based Scraping',
          summary: {
            totalSources: isDirectUrl ? 1 : searchResults.length,
            successfulExtractions: extractedContents.length,
            totalWords: analysis?.totalWords || extractedContents.reduce((sum, content) => sum + content.content.split(' ').length, 0),
            mode,
            dataType: isDirectUrl ? 'Company Directory' : 'General Content'
          },
          sources: extractedContents.map(content => ({
            url: content.url,
            title: content.title,
            wordCount: content.content.split(' ').length,
            description: content.metadata?.description || 'No description available',            author: (content.metadata as Record<string, unknown>)?.author as string || 'Unknown',
            publishDate: (content.metadata as Record<string, unknown>)?.publishDate as string || 'Unknown',
            // Company-specific fields
            ...(isDirectUrl ? {
              companyName: content.metadata?.name || content.title,
              website: content.metadata?.website || content.url,
              employees: content.metadata?.employees || 'Unknown',
              industry: content.metadata?.industry || 'Unknown',
              location: content.metadata?.location || 'Unknown'
            } : {})
          })),
          analysis,
          rawContent: mode === 'deep-scraping' ? extractedContents : undefined,
          // Add company data table for direct URL scraping
          ...(isDirectUrl ? {
            companyData: extractedContents.map(content => ({
              'Company Name': content.metadata?.name || content.title,
              'Website': content.metadata?.website || content.url,
              'Employees': content.metadata?.employees || 'Unknown',
              'Industry': content.metadata?.industry || 'Unknown',
              'Location': content.metadata?.location || 'Unknown',
              'Description': content.metadata?.description || 'No description'
            }))
          } : {})
        };

        scraper.updateStep(reportStepId, 'completed', {
          analysisComplete: true,
          extractedData: reportData
        } as StepData);

        results.steps = scraper.getSteps();
        results.reportData = reportData as ReportData;

        return NextResponse.json(results);

      } catch (error) {
        console.error('Scraping error:', error);
        
        // If we have steps, mark the last one as error
        const steps = scraper.getSteps();
        if (steps.length > 0) {
          const lastStep = steps[steps.length - 1];
          if (lastStep.status === 'in-progress') {
            scraper.updateStep(lastStep.id, 'error');
          }
        }

        return NextResponse.json({
          success: false,
          steps: scraper.getSteps(),
          error: error instanceof Error ? error.message : 'Unknown scraping error'
        }, { status: 500 });
      }    } catch (error) {
      console.error('[POST] Request processing error:', error);
      return NextResponse.json({
        success: false,
        error: 'Invalid request format',
        details: error instanceof Error ? error.message : 'Unknown error'
      }, { status: 400 });
    }
  };
  try {
    return await Promise.race([mainProcess(), timeoutPromise]) as NextResponse;
  } catch (error) {
    console.error('POST request timed out or failed:', error);
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'Request timeout or unknown error'
    }, { status: 408 });
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'Tetika Scraping API',
    version: '1.0.0',
    endpoints: {
      'POST /api/scraping': 'Perform enhanced web scraping with thinking process'
    },
    supportedModes: ['quick-scraping', 'deep-scraping'],
    features: [
      'Multi-engine search',
      'Content extraction',
      'Keyword analysis', 
      'Thinking process tracking',
      'Downloadable reports'
    ]
  });
}
